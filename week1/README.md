# Reinforcement Learning Agents Comparison
**FetchPickAndPlace-v4 with Sparse Rewards**

Compares three RL algorithms (PPO, SAC, SAC+HER) on a robotic pick-and-place task with sparse rewards, running on CUDA GPU with human visualization.

## Quick Start

```bash
# 1. Activate environment
.\arel\Scripts\Activate.ps1

# 2. Run all experiments (~6-8 hours with GPU)
python run_all_experiments.py

# 3. Plot results
python plot_results.py
```

## Project Files

| File | Description |
|------|-------------|
| `week1.py` | Environment setup (FetchPickAndPlace-v4, sparse rewards, human visualization) |
| `run_all_experiments.py` | Master script - runs all 3 agents sequentially |
| `run_ppo.py` | Train PPO agent (on-policy, 1M timesteps) |
| `run_sac.py` | Train SAC agent (off-policy, 1M timesteps) |
| `run_sac_her.py` | Train SAC+HER agent (off-policy + goal relabeling, 1M timesteps) |
| `plot_results.py` | Generate comparison plots and learning analysis |
| `verify_setup.py` | Check dependencies and CUDA availability |

## Configuration

- **Hardware**: CUDA GPU (NVIDIA RTX 4060) for accelerated training
- **Visualization**: Human rendering mode enabled (watch training live)
- **Seed**: 42 (reproducibility)
- **Training**: 1M timesteps per agent, 5 parallel environments
- **Logging**: TensorBoard logs + config files in `results/` directory

## Algorithms

| Algorithm | Type | Sample Efficiency | Expected Performance |
|-----------|------|-------------------|---------------------|
| **PPO** | On-policy | Medium | Gradual, stable learning |
| **SAC** | Off-policy | High | Faster than PPO, smoother curves |
| **SAC+HER** | Off-policy + Goal relabeling | Very High | **Best** - rapid learning via hindsight |

**Key Difference**: HER converts failed trajectories into successful training examples by relabeling goals, making it ideal for sparse reward tasks.

## Output Structure

```
results/
├── PPO/YYYYMMDD_HHMMSS/        # PPO results
├── SAC/YYYYMMDD_HHMMSS/        # SAC results  
├── SAC_HER/YYYYMMDD_HHMMSS/    # SAC+HER results
│   ├── config.txt              # Training configuration
│   ├── model.pkl               # Trained model weights
│   └── events.out.tfevents.*   # TensorBoard logs
└── comparison_plots.png         # Generated by plot_results.py
```

## View Results

```bash
# TensorBoard (interactive visualization)
tensorboard --logdir=results/

# Or generate static comparison plots
python plot_results.py
```

---

**Environment**: FetchPickAndPlace-v4 (Sparse Rewards) | **GPU**: CUDA Enabled | **Seed**: 42 | **Training**: ~6-8 hours
